{
	"name": "Notebook 1",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "MLSparkPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "3133db07-c13a-407a-ae35-1278d8183d07"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8c95e5bc-6519-484b-a2e5-c1efd1fdd75f/resourceGroups/myadf/providers/Microsoft.Synapse/workspaces/myadfasas/bigDataPools/MLSparkPool",
				"name": "MLSparkPool",
				"type": "Spark",
				"endpoint": "https://myadfasas.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/MLSparkPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"import sklearn_pandas as skp\r\n",
					"import sklearn\r\n",
					"import scipy\r\n",
					"import numpy\r\n",
					"from sklearn import linear_model\r\n",
					"from sklearn.metrics import mean_absolute_error\r\n",
					"from sklearn.preprocessing import OneHotEncoder\r\n",
					"from sklearn.preprocessing import OrdinalEncoder\r\n",
					"from sklearn.preprocessing import LabelEncoder\r\n",
					"from sklearn.model_selection import cross_val_score\r\n",
					"from sklearn.model_selection import RepeatedKFold\r\n",
					"from sklearn.ensemble import RandomForestRegressor\r\n",
					"from sklearn.ensemble import ExtraTreesRegressor\r\n",
					"from sklearn.experimental import enable_hist_gradient_boosting\r\n",
					"from sklearn.ensemble import HistGradientBoostingRegressor\r\n",
					"from sklearn.ensemble import GradientBoostingRegressor\r\n",
					"from window_ops.rolling import rolling_mean, rolling_max, rolling_min\r\n",
					"from mlforecast import MLForecast\r\n",
					"from mlforecast.utils import generate_daily_series\r\n",
					"from sklearn.impute import SimpleImputer\r\n",
					"from sklearn.pipeline import make_pipeline\r\n",
					"from xgboost import XGBRegressor\r\n",
					"from sklearn.linear_model import LinearRegression\r\n",
					"from sklearn.preprocessing import MinMaxScaler\r\n",
					"from sklearn.metrics import accuracy_score\r\n",
					"from sklearn.neural_network import MLPRegressor\r\n",
					"from sklearn.metrics import mean_squared_error\r\n",
					"from sklearn.impute import SimpleImputer\r\n",
					"from sklearn.pipeline import make_pipeline\r\n",
					"from xgboost import XGBRegressor\r\n",
					"from sklearn.linear_model import LinearRegression\r\n",
					"from sklearn.preprocessing import MinMaxScaler\r\n",
					"from sklearn.metrics import accuracy_score\r\n",
					"from sklearn.neural_network import MLPRegressor\r\n",
					"from sklearn.metrics import mean_squared_error\r\n",
					"from numpy import ravel\r\n",
					"from sklearn.preprocessing import StandardScaler\r\n",
					"from numpy import unique\r\n",
					"from numpy import argmax\r\n",
					"# The servername is in the format \"jdbc:sqlserver://<AzureSQLServerName>.database.windows.net:1433\"\r\n",
					"servername = \"jdbc:sqlserver://myadfasas.sql.azuresynapse.net:1433\"\r\n",
					"dbname = \"dedicatedsqlpool1\"\r\n",
					"url = servername + \";\" + \"databaseName=\" + dbname + \";\"\r\n",
					"dbtable = \"dbo.pecan_submit\"\r\n",
					"user = \"sqladminuser\"\r\n",
					"password=\"Stegopuss3$\"\r\n",
					"\r\n",
					"print(\"read data from SQL server table  \")\r\n",
					"df_init = spark.read \\\r\n",
					"        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
					"        .option(\"url\", url) \\\r\n",
					"        .option(\"dbtable\", dbtable) \\\r\n",
					"        .option(\"user\", user) \\\r\n",
					"        .option(\"password\", password).load()\r\n",
					"\r\n",
					"#df_init.show(5)\r\n",
					"\r\n",
					""
				],
				"execution_count": 76
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"weight_column_names=[\"\"]\r\n",
					"label_column_names=[\"EXECUTION_TIME\"]\r\n",
					"feature_column_names=[\"QUERY_TYPE\",\"PARTITIONS_SCANNED\",\"BYTES_SPILLED_TO_LOCAL_STORAGE\",\"BYTES_SPILLED_TO_REMOTE_STORAGE\",\"BYTES_SCANNED\",\"PERCENTAGE_SCANNED_FROM_CACHE\",\"BYTES_WRITTEN\"]\r\n",
					"OUTPUT_COLUMNS = [\"PREDICTED_EXECUTION_TIME\"]\r\n",
					"data2=df_init.select(\"MARKER\",\"ENTITY_ID\",\"EXECUTION_TIME\",\"QUERY_TYPE\",\"PARTITIONS_SCANNED\",\"BYTES_SPILLED_TO_LOCAL_STORAGE\",\"BYTES_SPILLED_TO_REMOTE_STORAGE\",\"BYTES_SCANNED\",\"PERCENTAGE_SCANNED_FROM_CACHE\",\"BYTES_WRITTEN\").distinct()\r\n",
					"data2=data2.toDF(\"DS\",\"UNIQUE_ID\",\"Y\",\"QUERY_TYPE\",\"PARTITIONS_SCANNED\",\"BYTES_SPILLED_TO_LOCAL_STORAGE\",\"BYTES_SPILLED_TO_REMOTE_STORAGE\",\"BYTES_SCANNED\",\"PERCENTAGE_SCANNED_FROM_CACHE\",\"BYTES_WRITTEN\")\r\n",
					"data2.show(3)\r\n",
					"data2=data2.toPandas()\r\n",
					"data2[\"QUERY_TYPE\"] = data2[\"QUERY_TYPE\"].astype('category')\r\n",
					"data2[\"QUERY_TYPE_CAT\"] = data2[\"QUERY_TYPE\"].cat.codes\r\n",
					"data2 = data2.drop('QUERY_TYPE', axis=1)\r\n",
					"#print(data2.dtypes)\r\n",
					"data2=spark.createDataFrame(data2)\r\n",
					"#print(data2[['DS', 'UNIQUE_ID','QUERY_TYPE_CAT','PARTITIONS_SCANNED']])\r\n",
					"data20=data2.select('DS', 'UNIQUE_ID','QUERY_TYPE_CAT','PARTITIONS_SCANNED')\r\n",
					"data20.show(5)\r\n",
					"\r\n",
					""
				],
				"execution_count": 77
			}
		]
	}
}