{
	"name": "Notebook 1",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "MLSparkPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d2fe19c3-10ed-4b9f-966b-e8dd8cb0fb15"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8c95e5bc-6519-484b-a2e5-c1efd1fdd75f/resourceGroups/myadf/providers/Microsoft.Synapse/workspaces/myadfasas/bigDataPools/MLSparkPool",
				"name": "MLSparkPool",
				"type": "Spark",
				"endpoint": "https://myadfasas.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/MLSparkPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"import sklearn_pandas as skp\r\n",
					"import sklearn\r\n",
					"import scipy\r\n",
					"import numpy\r\n",
					"import lightgbm\r\n",
					"import pandas as pd\r\n",
					"import tensorflow as tf\r\n",
					"import keras_applications\r\n",
					"from pyspark.sql import functions as F\r\n",
					"from pyspark.sql.types import LongType\r\n",
					"from pyspark.ml.feature import StringIndexer\r\n",
					"from sklearn import linear_model\r\n",
					"from sklearn.metrics import mean_absolute_error\r\n",
					"from sklearn.preprocessing import OneHotEncoder\r\n",
					"from sklearn.preprocessing import OrdinalEncoder\r\n",
					"from sklearn.preprocessing import LabelEncoder\r\n",
					"from sklearn.model_selection import cross_val_score\r\n",
					"from sklearn.model_selection import RepeatedKFold\r\n",
					"from sklearn.ensemble import RandomForestRegressor\r\n",
					"from sklearn.ensemble import ExtraTreesRegressor\r\n",
					"from sklearn.experimental import enable_hist_gradient_boosting\r\n",
					"from sklearn.ensemble import HistGradientBoostingRegressor\r\n",
					"from sklearn.ensemble import GradientBoostingRegressor\r\n",
					"#from window_ops.rolling import rolling_mean, rolling_max, rolling_min\r\n",
					"from mlforecast import MLForecast\r\n",
					"from mlforecast.utils import generate_daily_series\r\n",
					"from mlforecast.target_transforms import Differences\r\n",
					"from sklearn.impute import SimpleImputer\r\n",
					"from sklearn.pipeline import make_pipeline\r\n",
					"from xgboost import XGBRegressor\r\n",
					"from sklearn.linear_model import LinearRegression\r\n",
					"from sklearn.preprocessing import MinMaxScaler\r\n",
					"from sklearn.metrics import accuracy_score\r\n",
					"from sklearn.neural_network import MLPRegressor\r\n",
					"from sklearn.metrics import mean_squared_error\r\n",
					"from sklearn.impute import SimpleImputer\r\n",
					"from sklearn.pipeline import make_pipeline\r\n",
					"from xgboost import XGBRegressor\r\n",
					"from sklearn.linear_model import LinearRegression\r\n",
					"from sklearn.preprocessing import MinMaxScaler\r\n",
					"from sklearn.metrics import accuracy_score\r\n",
					"from sklearn.neural_network import MLPRegressor\r\n",
					"from sklearn.metrics import mean_squared_error\r\n",
					"from numpy import ravel\r\n",
					"from sklearn.preprocessing import StandardScaler\r\n",
					"from numpy import unique\r\n",
					"from numpy import argmax\r\n",
					"# The servername is in the format \"jdbc:sqlserver://<AzureSQLServerName>.database.windows.net:1433\"\r\n",
					"servername = \"jdbc:sqlserver://myadfasas.sql.azuresynapse.net:1433\"\r\n",
					"dbname = \"dedicatedsqlpool1\"\r\n",
					"url = servername + \";\" + \"databaseName=\" + dbname + \";\"\r\n",
					"dbtable = \"dbo.pecan_submit\"\r\n",
					"user = \"sqladminuser\"\r\n",
					"password=\"Stegopuss3$\"\r\n",
					"\r\n",
					"print(\"read data from SQL server table  \")\r\n",
					"df_init = spark.read \\\r\n",
					"        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
					"        .option(\"url\", url) \\\r\n",
					"        .option(\"dbtable\", dbtable) \\\r\n",
					"        .option(\"user\", user) \\\r\n",
					"        .option(\"password\", password).load()\r\n",
					"\r\n",
					"#df_init.show(5)\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					""
				],
				"execution_count": 317
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"weight_column_names=[\"\"]\r\n",
					"label_column_names=[\"EXECUTION_TIME\"]\r\n",
					"feature_column_names=[\"QUERY_TYPE\",\"PARTITIONS_SCANNED\",\"BYTES_SPILLED_TO_LOCAL_STORAGE\",\"BYTES_SPILLED_TO_REMOTE_STORAGE\",\"BYTES_SCANNED\",\"PERCENTAGE_SCANNED_FROM_CACHE\",\"BYTES_WRITTEN\"]\r\n",
					"OUTPUT_COLUMNS = [\"PREDICTED_EXECUTION_TIME\"]\r\n",
					"data2=df_init.select(F.to_timestamp(F.col(\"MARKER\")/1000),\"ENTITY_ID\",\"EXECUTION_TIME\",\"QUERY_TYPE\",\"PARTITIONS_SCANNED\",\"BYTES_SPILLED_TO_LOCAL_STORAGE\",\"BYTES_SPILLED_TO_REMOTE_STORAGE\",\"BYTES_SCANNED\",\"PERCENTAGE_SCANNED_FROM_CACHE\",\"BYTES_WRITTEN\").distinct()\r\n",
					"#data2=df_init.select(F.col(\"MARKER\")/1000,\"ENTITY_ID\",\"EXECUTION_TIME\",\"QUERY_TYPE\",\"PARTITIONS_SCANNED\",\"BYTES_SPILLED_TO_LOCAL_STORAGE\",\"BYTES_SPILLED_TO_REMOTE_STORAGE\",\"BYTES_SCANNED\",\"PERCENTAGE_SCANNED_FROM_CACHE\",\"BYTES_WRITTEN\").distinct()\r\n",
					"data2=data2.toDF(\"DS\",\"UNIQUE_ID\",\"Y\",\"QUERY_TYPE\",\"PARTITIONS_SCANNED\",\"BYTES_SPILLED_TO_LOCAL_STORAGE\",\"BYTES_SPILLED_TO_REMOTE_STORAGE\",\"BYTES_SCANNED\",\"PERCENTAGE_SCANNED_FROM_CACHE\",\"BYTES_WRITTEN\")\r\n",
					"#data2=data2.withColumn(\"DS\", data2[\"DS\"].cast(LongType()))\r\n",
					"#data2=data2.toPandas()\r\n",
					"#data2[\"QUERY_TYPE\"] = data2[\"QUERY_TYPE\"].astype('category')\r\n",
					"#data2[\"QUERY_TYPE_CAT\"] = data2[\"QUERY_TYPE\"].cat.codes\r\n",
					"indexer = StringIndexer(inputCol=\"QUERY_TYPE\", outputCol=\"QUERY_TYPE_CAT\")\r\n",
					"data2 = indexer.fit(data2).transform(data2)\r\n",
					"data2 = data2.drop('QUERY_TYPE')\r\n",
					"#print(data2.dtypes)\r\n",
					"data12=data2.toPandas()\r\n",
					"#data2=spark.createDataFrame(data2)\r\n",
					"print(data12[['DS', 'UNIQUE_ID','QUERY_TYPE_CAT','PARTITIONS_SCANNED']])\r\n",
					"#data20=data2.select('DS', 'UNIQUE_ID','QUERY_TYPE_CAT','PARTITIONS_SCANNED')\r\n",
					"#data20.show(5)\r\n",
					"#print(data2.count())\r\n",
					"data2.show(3)\r\n",
					"\r\n",
					"\r\n",
					"train, valid = data2.randomSplit([0.7, 0.3], seed=42)\r\n",
					"\r\n",
					"\r\n",
					"#train.dropna(how='any')\r\n",
					"#valid.dropna(how='any')\r\n",
					"Y=valid[['UNIQUE_ID','DS','Y']]\r\n",
					"\r\n",
					"train=train.to_pandas_on_spark()\r\n",
					"\r\n",
					"Y=Y.to_pandas_on_spark()\r\n",
					"\r\n",
					"#print(f\"train {train.count()}  valid {valid.count()}\")\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"Y=valid[['UNIQUE_ID','DS','Y']]\r\n",
					"\r\n",
					"x_train=train.drop('Y')\r\n",
					"\r\n",
					"x_test=valid.drop('Y')\r\n",
					"\r\n",
					"y_train=train[['Y']]\r\n",
					"\r\n",
					"x_train=x_train.drop('UNIQUE_ID')\r\n",
					"\r\n",
					"x_test=x_test.drop('UNIQUE_ID')\r\n",
					"\r\n",
					"#models = [make_pipeline(SimpleImputer(),RandomForestRegressor(random_state=0, n_estimators=600)),XGBRegressor(random_state=0,n_estimators=600),lightgbm.LGBMRegressor(random_state=0,n_estimators=600),HistGradientBoostingRegressor()]\r\n",
					"\r\n",
					"#model = MLForecast(models=models,\r\n",
					"#                   freq='D',\r\n",
					"#                  lags=[1,2],\r\n",
					"#                  lag_transforms={\r\n",
					"#                      1: [(rolling_mean, 1),(rolling_min, 1), (rolling_max, 2)],\r\n",
					"#                  },\r\n",
					"#                  date_features=['day'],\r\n",
					"#                   num_threads=6)\r\n",
					"\r\n",
					"model = MLForecast(\r\n",
					"    models=XGBRegressor(),\r\n",
					"    freq='D',  # our series has a daily frequency\r\n",
					"    lags=[1],\r\n",
					"    target_transforms=[Differences([1])],\r\n",
					")\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"#model.fit(train, id_col='UNIQUE_ID', time_col='DS', target_col='Y',static_features=[])\r\n",
					"model.fit(train)\r\n",
					"\r\n",
					""
				],
				"execution_count": 318
			}
		]
	}
}